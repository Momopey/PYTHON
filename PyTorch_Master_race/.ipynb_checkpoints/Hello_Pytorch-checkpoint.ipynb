{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f2c16e4b2de9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# from __future__ import print_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "import torch\n",
    "x = torch.rand(5, 3,2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3842, 0.7761],\n",
      "        [0.7172, 0.5198],\n",
      "        [0.9828, 0.2298],\n",
      "        [0.7441, 0.9574],\n",
      "        [0.6274, 0.2878]])\n"
     ]
    }
   ],
   "source": [
    "y= torch.rand([5,2])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch as torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train= True, download= True, \n",
    "                       transform= transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train= False, download= True, \n",
    "                      transform= transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train,batch_size=10,shuffle=True) \n",
    "testset = torch.utils.data.DataLoader(test,batch_size=10,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    print(data[0][0].shape)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y)=data[0][0],data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAOI0lEQVR4nO3df6xU9ZnH8c8DXNG91S0UobdCi0Wyrd1W2t7FWlpXo7WWTRb9w0ayaWjj9naruNqYpta2qX+4idsWjVurybWwhcbakvqLbHArC+66jZT1Qig/ZC1qUJC7oCUqNNvLBZ794x42F5zznWHOmTkDz/uV3MzMeebMeRz53DN3vuecr7m7AJz6xlTdAID2IOxAEIQdCIKwA0EQdiCIce3c2Gk23k9Xdzs3CYTyR/1BB33IatUKhd3MrpR0j6Sxkn7s7nemnn+6unWhXVZkkwAS1vnq3FrTH+PNbKykH0n6nKTzJc03s/ObfT0ArVXkb/bZkl5w95fc/aCkn0uaV05bAMpWJOznSNo56vGubNkxzKzPzAbMbGBYQwU2B6CIImGv9SXA2469dfd+d+91994ujS+wOQBFFAn7LknTRj2eKml3sXYAtEqRsD8raaaZnWtmp0m6VtKKctoCULamh97c/ZCZLZT0K40MvS1x962ldQagVIXG2d19paSVJfUCoIU4XBYIgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQRKEpm81sh6T9kg5LOuTuvWU0BaB8hcKeudTdXy/hdQC0EB/jgSCKht0lPWlm682sr9YTzKzPzAbMbGBYQwU3B6BZRT/Gz3H33WY2WdIqM/tvd3969BPcvV9SvySdZRO94PYANKnQnt3dd2e3eyU9Kml2GU0BKF/TYTezbjM78+h9SVdI2lJWYwDKVeRj/BRJj5rZ0df5mbv/ayldAShd02F395ckXVBiLwBaiKE3IAjCDgRB2IEgCDsQBGEHgijjRJgQDlxzYW7tl4sWJdf97cF3JevXP/M3yfp59x9O1sdt25FbO/zGm8l1EQd7diAIwg4EQdiBIAg7EARhB4Ig7EAQhB0Iwtzbd/GYs2yiX2iXtW17J2Ls2Wcn65c/9WJu7YYJz5fdzjHG1PmdfMfrH8mtLVs7J7nu1F9Zsv4nj65L1tFZ1vlqveX7av5PZc8OBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0Ewzp7Z9c1PJusbFt7T9GvvP3IwWX9uuDtZv2h8+nz2Izpywj0dVa+354fPSNa3Dk1N1v/xib/Orf3ZA79Prqt96XPxD+/Zm14/IMbZARB2IArCDgRB2IEgCDsQBGEHgiDsQBCMs2f+5dX1yXqRsewPL/v7ZP3cb65N1n//5YuS9bdm5Nf+8tJNyXXvnfrvyXo99c61L/K+rRvqStZvueOryfrEJen39VRUaJzdzJaY2V4z2zJq2UQzW2Vm27PbCWU2DKB8jXyM/4mkK49bdquk1e4+U9Lq7DGADlY37O7+tKR9xy2eJ2lpdn+ppKtK7gtAyZr9gm6Kuw9KUnY7Oe+JZtZnZgNmNjCsoSY3B6Coln8b7+797t7r7r1dGt/qzQHI0WzY95hZjyRlt5x+BHS4ZsO+QtKC7P4CSY+X0w6AVqk7zm5mD0m6RNIkSXskfVfSY5KWS3qvpFckXePux3+J9zadPM6+fenHkvUfzvlZbu3yM/Yn1z1/+Y3J+nlf+02yXoR1nZas+8c/UOj1/zjp9GR95zWHcmvjxufXJGnzpxc31dNRH3xkYW5t5o2n5vXwU+Ps4+qt7O7zc0qdmVoANXG4LBAEYQeCIOxAEIQdCIKwA0FwimuDpqw9K7e2+L1PJdf90svp/+bXPvlGUz2d6gZvSV/ee83N30/WJ43Nv0T3xdf3Jdc947H/StY7FZeSBkDYgSgIOxAEYQeCIOxAEIQdCIKwA0HUPesNI454/u/FepdLPuI1hz1RR8+iZ5L1ZV/6cLJ+44TtZbZz0mPPDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBMM7eoMFv58+L/KG/Sl+OuXtn+nfqu5UeT26lMWeemawf2Z++TDZOHuzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIxtkbNG7N+tzajDVtbKSGce+bllvbcVf+9e4l6eoZm5L1J19NH0MwxtLzDqTO5X9z46TkupMH0tcJeE/X8mQdx6q7ZzezJWa218y2jFp2u5m9amYbs5+5rW0TQFGNfIz/iaQrayy/291nZT8ry20LQNnqht3dn5a0rw29AGihIl/QLTSzTdnH/Al5TzKzPjMbMLOBYQ0V2ByAIpoN+/2SZkiaJWlQ0qK8J7p7v7v3untvl8Y3uTkARTUVdnff4+6H3f2IpAckzS63LQBlayrsZtYz6uHVkrbkPRdAZ6g7P7uZPSTpEkmTJO2R9N3s8SxJLmmHpK+4+2C9jZ3M87N3shfu/kRu7bnP/7Cl2x5TZ39R75r6rZTq7eED6TH+3cO5X0M15L4nPpusz/j62kKvnyc1P3vdg2rcfX6NxYsLdwWgrThcFgiCsANBEHYgCMIOBEHYgSA4xfUUcN7XfpNb+4tdNyXXPVznoMbp//xSsn5o8H/SL1DAmyvPS9b/84JfJOvX77o4t/bM4xc01VOjOvFYUfbsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAE4+ynuJ5FxaaDPlRSH7WM6e5O1nvP3pms1zt99pUL/5Bbm1rhNNlVYc8OBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0Ewzo7KvPjtjyTrj73nn5L1617+TJ0tvHGCHZ3a2LMDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCMs6MyV1y+IVnfdjB9vvq+v51cZwuMs49Wd89uZtPM7Ckz22ZmW83spmz5RDNbZWbbs9tiE1oDaKlGPsYfknSLu39Q0ick3WBm50u6VdJqd58paXX2GECHqht2dx909w3Z/f2Stkk6R9I8SUuzpy2VdFWrmgRQ3Al9QWdm0yV9VNI6SVPcfVAa+YUgqeYfUGbWZ2YDZjYwrKFi3QJoWsNhN7N3SHpY0s3u/laj67l7v7v3untvV0dOdwfE0FDYzaxLI0F/0N0fyRbvMbOerN4jaW9rWgRQhrpDb2ZmkhZL2ubud40qrZC0QNKd2e3jLekQJ7Udd1yUW1vec1duTZK++vLcZP3wc79rqqeoGhlnnyPpC5I2m9nGbNltGgn5cjO7TtIrkq5pTYsAylA37O7+a0mWU76s3HYAtAqHywJBEHYgCMIOBEHYgSAIOxAEp7iikHHvn56s33Ptktza2NxBnhHr13wgWZ+utck6jsWeHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCYJwdhbz4xZ5k/dIzDuTWPvQff5dcd8Z3GEcvE3t2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCcXYk/e+82cn65uvuTdZTky5Pemf+GDzKx54dCIKwA0EQdiAIwg4EQdiBIAg7EARhB4JoZH72aZKWSXq3RoZN+939HjO7XdKXJb2WPfU2d1/ZqkZRjav+4d8Krb9uqCu31v2DPy302jgxjRxUc0jSLe6+wczOlLTezFZltbvd/Qetaw9AWRqZn31Q0mB2f7+ZbZN0TqsbA1CuE/qb3cymS/qopHXZooVmtsnMlpjZhJx1+sxswMwGhjVUqFkAzWs47Gb2DkkPS7rZ3d+SdL+kGZJmaWTPv6jWeu7e7+697t7bpfEltAygGQ2F3cy6NBL0B939EUly9z3uftjdj0h6QFL6jAkAlaobdjMzSYslbXP3u0YtH31Z0aslbSm/PQBlaeTb+DmSviBps5ltzJbdJmm+mc2S5JJ2SPpKSzpEpe574rPJ+r2TL0nWpz+Yvz/pWjPQTEtoUiPfxv9aqjmRNmPqwEmEI+iAIAg7EARhB4Ig7EAQhB0IgrADQXApaSTN+DrTJp8q2LMDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBDm7u3bmNlrkl4etWiSpNfb1sCJ6dTeOrUvid6aVWZv73P3s2sV2hr2t23cbMDdeytrIKFTe+vUviR6a1a7euNjPBAEYQeCqDrs/RVvP6VTe+vUviR6a1Zbeqv0b3YA7VP1nh1AmxB2IIhKwm5mV5rZ82b2gpndWkUPecxsh5ltNrONZlbphc2zOfT2mtmWUcsmmtkqM9ue3dacY6+i3m43s1ez926jmc2tqLdpZvaUmW0zs61mdlO2vNL3LtFXW963tv/NbmZjJf1O0mck7ZL0rKT57v5cWxvJYWY7JPW6e+UHYJjZxZIOSFrm7n+eLfuepH3ufmf2i3KCu3+jQ3q7XdKBqqfxzmYr6hk9zbikqyR9URW+d4m+Pq82vG9V7NlnS3rB3V9y94OSfi5pXgV9dDx3f1rSvuMWz5O0NLu/VCP/WNoup7eO4O6D7r4hu79f0tFpxit97xJ9tUUVYT9H0s5Rj3eps+Z7d0lPmtl6M+urupkaprj7oDTyj0fS5Ir7OV7dabzb6bhpxjvmvWtm+vOiqgh7ramkOmn8b467f0zS5yTdkH1cRWMamsa7XWpMM94Rmp3+vKgqwr5L0rRRj6dK2l1BHzW5++7sdq+kR9V5U1HvOTqDbna7t+J+/l8nTeNda5pxdcB7V+X051WE/VlJM83sXDM7TdK1klZU0MfbmFl39sWJzKxb0hXqvKmoV0hakN1fIOnxCns5RqdM4503zbgqfu8qn/7c3dv+I2muRr6Rf1HSt6roIaev90v6bfaztereJD2kkY91wxr5RHSdpHdJWi1pe3Y7sYN6+6mkzZI2aSRYPRX19imN/Gm4SdLG7Gdu1e9doq+2vG8cLgsEwRF0QBCEHQiCsANBEHYgCMIOBEHYgSAIOxDE/wEd4lWP6RIGYgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(data[0][1].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input balance: \n",
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n",
      "0 :9.871666666666666 %\n",
      "1 :11.236666666666666 %\n",
      "2 :9.93 %\n",
      "3 :10.218333333333334 %\n",
      "4 :9.736666666666666 %\n",
      "5 :9.035 %\n",
      "6 :9.863333333333333 %\n",
      "7 :10.441666666666666 %\n",
      "8 :9.751666666666667 %\n",
      "9 :9.915000000000001 %\n"
     ]
    }
   ],
   "source": [
    "print(\" Input balance: \")\n",
    "total = 0\n",
    "counter_dict= {0:0,1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0}\n",
    "\n",
    "for data in trainset:\n",
    "    Xs, ys= data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)]+=1\n",
    "        total+=1\n",
    "\n",
    "print(counter_dict)\n",
    "\n",
    "for i in counter_dict:\n",
    "    print( str(i) +\" :\"+ str(counter_dict[i]/total*100) + \" %\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1= nn.Linear(28*28, 64)\n",
    "        self.fc2= nn.Linear(64, 64)\n",
    "        self.fc3= nn.Linear(64, 64)\n",
    "        self.fc4= nn.Linear(64, 10 )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x= nnf.relu(self.fc1(x))\n",
    "        x= nnf.relu(self.fc2(x))\n",
    "        x= nnf.relu(self.fc3(x))\n",
    "        x= self.fc4(x)\n",
    "        return nnf.log_softmax(x,dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net= Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0241, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1948, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0009, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer= optim.Adam(net.parameters(),lr=0.001)\n",
    "\n",
    "# iterate\n",
    "EPOCHS =3 # #full passes\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        # data is a batch of featcuresets and labels\n",
    "        X,y = data\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1,28*28))\n",
    "        loss= nnf.nll_loss(output,y)\n",
    "        loss.backward() # MAGIC\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.03166666666667 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net (X.view(-1,28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print( \"Accuracy: \"+ str( correct/total*100.0)+ \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1eea9ebb3a24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpredict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n=8\n",
    "for n in range(0,10):\n",
    "    image=X[n].view(-1,28*28)\n",
    "    out=net(image)\n",
    "    predict=torch.argmax(out)\n",
    "    if(predict!=y[n]):\n",
    "        print(predict)\n",
    "        plt.imshow(X[n].view(28,28))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        print(out[0,0])\n",
    "        print(y[n])\n",
    "# print(torch.argmax())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray,(5,5),5000)\n",
    "    thresh = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "            cv2.THRESH_BINARY,11,2)\n",
    "    clean =   cv2.GaussianBlur(thresh,(11,11),3000)\n",
    "    \n",
    "    # Our operations on the frame come hereq#     gray = cv2.cvtColor(frame, cv2.COLOR_RGB)\n",
    "#     print(inp)\n",
    " # Display the resulting frame\n",
    "    inp= cv2.resize(clean,(960,400))\n",
    "    \n",
    "    data= cv2.resize(clean,(240,100))\n",
    "    data= 1- data/255\n",
    "#     print(data.shape)\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    number_data= torch.zeros((240//10,100//10,2))\n",
    "    for x in range(0,(240-28)//10):\n",
    "        for y in range(0,(100-28)//10):\n",
    "#             print(str(x)+\"  \"+str(y))\n",
    "#             print(str(x+28)+\"  \"+str(y+28))\n",
    "            image_slice= data[x*10:x*10+28,y*10:y*10+28]\n",
    "#             print(str(x)+\" \"+str(y))\n",
    "#             print(str(x*10)+\"  \"+str(y*10))\n",
    "#             print(str(x*10+28)+\"  \"+str(y*10+28))\n",
    "   \n",
    "#             print(image_slice.shape)\n",
    "            if(image_slice.shape == (28,28)):\n",
    "                flatten=torch.Tensor(image_slice).view(-1,28*28)\n",
    "#                 print(flatten.shape)\n",
    "                predictions =net(flatten)\n",
    "                num= torch.argmax(predictions)\n",
    "                number_data[x,y,0]=num\n",
    "    #             print(predictions)\n",
    "                number_data[x,y,1]=predictions[0,num]\n",
    "#                 print(predictions)\n",
    "                if(predictions[0,num]>-0.01):\n",
    "                    cv2.putText(inp,str(num.item()), \n",
    "                        (x*10*4,y*10*4), #bottomLeftCornerOfText =\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,  # font                   = \n",
    "                        0.5, # fontScale = \n",
    "                        (0,0,0), #fontColor              = \n",
    "                         2) #lineType\n",
    "                    cv2.rectangle(inp,(x*10*4,y*10*4),(x*10*4+28*4,y*10*4+28*4),(0,255,0),3)\n",
    "                count+=1\n",
    "          #  else:\n",
    "#                 print(\"CHARSAHS\")\n",
    "#     print(count)\n",
    "    \n",
    "\n",
    "\n",
    "#     image= np.hstack((inp, np.pad(data,150)*255))\n",
    "    cv2.imshow('frame',inp)\n",
    "    cv2.imshow('Data',data)\n",
    "    X,Y= 10,10\n",
    "    cv2.imshow('Slice',data[X:X+28,Y:Y+28])\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_RGB)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',frame)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "t= torch.zeros(10,10)\n",
    "print(t[0,0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
